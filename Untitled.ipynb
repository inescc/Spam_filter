{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import packages \n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: enron in india  mark ,  two points .  1 . you probably know about it already . abhay mehta , the author of \" power  play \" , is on a  tour of the united states . please , take a look at the information about a  meeting last sunday  at stanford university . the web site address is given below . my wife went to  the presentation and told me it was quite critical about enron . about 40  people attended .  2 . i was approached by john martin , a professor of finance at baylor , to  write jointly an  academic paper on enron for a financial journal . he wanted to work on an  article on ebs .  i have suggested a different topic : enron - case study of a company  reinventing itself .  i made a few points to john :  a . enron ' s evolution did not just happen by accident .  it was a result of implementation of a far - reaching  strategy developed by the management .  b . in the process of its evolution enron changed its environment . i came up  with a term  \" proactive evolution \" , as opposed to \" reactive evolution . \"  c . the strategy included many elements , including emphasis on the quality  of human resources , changing corporate attitudes to risk taking and employee  empowerment .  d . there are very few companies that match enron ' s experience and  accomplishemnts .  the paper could become a standard reading at the mba courses on corporate  strategy  and would help greatly our recruiting efforts .  writing the paper would require interviews with ken , jeff and a few other key  players .  let me know what you thing about it . john is really excited about this paper .  vince  - - - - - - - - - - - - - - - - - - - - - - forwarded by vince j kaminski / hou / ect on 10 / 09 / 2000  08 : 07 am - - - - - - - - - - - - - - - - - - - - - - - - - - -  vkaminski @ aol . com on 10 / 07 / 2000 05 : 29 : 41 pm  to : vkamins @ enron . com  cc :  subject : enron  http : / / www . stanford . edu / group / sia /  stanford india association\n",
      "1965\n"
     ]
    }
   ],
   "source": [
    "## PREPARING ##\n",
    "\n",
    "# 2. Import data needed\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "#df_pos = df[df['spam']==1]\n",
    "\n",
    "# 3. Start to clean the data. In order not to let it crash, we work with one sentence\n",
    "text = np.random.choice(df['text'])\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #tokens without special character\n",
    "stop_words = set(stopwords.words('english')) #stop words remover\n",
    "ps = PorterStemmer() #stemming words\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizing words\n",
    "\n",
    "\n",
    "print(text)\n",
    "print(len(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: are you ready to get it ?  hello !  viagra is the # med to struggle with mens ' erectile dysfunction .  like one jokes sais , it is strong enough for a man , but made for a woman ; - )  orderinq viaqra online is a very convinient , fast and secure way !  miliions of people do it daily to save their privacy and money  order here . . . \n",
      "345\n"
     ]
    }
   ],
   "source": [
    "# 4. Remove numbers \n",
    "remv_numbers = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", text)\n",
    "print(remv_numbers)\n",
    "print(len(remv_numbers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', 'are', 'you', 'ready', 'to', 'get', 'it', 'hello', 'viagra', 'is', 'the', 'med', 'to', 'struggle', 'with', 'mens', 'erectile', 'dysfunction', 'like', 'one', 'jokes', 'sais', 'it', 'is', 'strong', 'enough', 'for', 'a', 'man', 'but', 'made', 'for', 'a', 'woman', 'orderinq', 'viaqra', 'online', 'is', 'a', 'very', 'convinient', 'fast', 'and', 'secure', 'way', 'miliions', 'of', 'people', 'do', 'it', 'daily', 'to', 'save', 'their', 'privacy', 'and', 'money', 'order', 'here']\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# 5. Remove numbers while tokenizing \n",
    "word_tokens = tokenizer.tokenize(remv_numbers)\n",
    "print(word_tokens)\n",
    "print(len(word_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--filtered sentence--\n",
      "['subject', 'readi', 'get', 'hello', 'viagra', 'med', 'struggl', 'men', 'erectil', 'dysfunct', 'like', 'one', 'joke', 'sai', 'strong', 'enough', 'man', 'made', 'woman', 'orderinq', 'viaqra', 'onlin', 'convini', 'fast', 'secur', 'way', 'miliion', 'peopl', 'daili', 'save', 'privaci', 'money', 'order']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# 6. Remove stop words while and stemming the rest of the words\n",
    "filtered_sentence = []\n",
    "\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(ps.stem(w))\n",
    "       \n",
    "        \n",
    "print ('--filtered sentence--')\n",
    "print(filtered_sentence)\n",
    "print(len(filtered_sentence))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--filtered sentence--\n",
      "['Subject', 'ready', 'get', 'hello', 'viagra', 'med', 'struggle', 'men', 'erectile', 'dysfunction', 'like', 'one', 'joke', 'sais', 'strong', 'enough', 'man', 'made', 'woman', 'orderinq', 'viaqra', 'online', 'convinient', 'fast', 'secure', 'way', 'miliions', 'people', 'daily', 'save', 'privacy', 'money', 'order']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# 6. Remove stop words while and stemming the rest of the words\n",
    "filtered_sentence = []\n",
    "\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        \n",
    "        filtered_sentence.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "print ('--filtered sentence--')\n",
    "print(filtered_sentence)\n",
    "print(len(filtered_sentence))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word         frequency\n",
      "-------------------------\n",
      "Subject              1\n",
      "ready                1\n",
      "get                  1\n",
      "hello                1\n",
      "viagra               1\n",
      "med                  1\n",
      "struggle             1\n",
      "men                  1\n",
      "erectile             1\n",
      "dysfunction           1\n",
      "like                 1\n",
      "one                  1\n",
      "joke                 1\n",
      "sais                 1\n",
      "strong               1\n",
      "...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'\"': 2,\n",
       " '$': 6,\n",
       " \"'\": 6,\n",
       " '(': 8,\n",
       " ')': 8,\n",
       " '*': 18,\n",
       " ',': 16,\n",
       " '-': 19,\n",
       " '.': 26,\n",
       " '/': 6,\n",
       " '00': 6,\n",
       " '000': 6,\n",
       " '1': 4,\n",
       " '10': 1,\n",
       " '11': 1,\n",
       " '2': 2,\n",
       " '2023': 1,\n",
       " '210': 1,\n",
       " '25': 1,\n",
       " '33': 1,\n",
       " '45': 1,\n",
       " '5': 4,\n",
       " '503': 2,\n",
       " '55': 1,\n",
       " '60': 1,\n",
       " '7': 2,\n",
       " '7130': 1,\n",
       " '722': 2,\n",
       " '79': 1,\n",
       " '99': 1,\n",
       " ':': 7,\n",
       " '>': 1,\n",
       " '?': 1,\n",
       " '@': 1,\n",
       " 'Subject:': 1,\n",
       " 'a': 6,\n",
       " 'about': 1,\n",
       " 'additional': 1,\n",
       " 'affiliate': 1,\n",
       " 'affiliates': 2,\n",
       " 'after': 1,\n",
       " 'again': 1,\n",
       " 'all': 1,\n",
       " 'also': 1,\n",
       " 'analysis': 1,\n",
       " 'and': 7,\n",
       " 'annual': 1,\n",
       " 'are': 1,\n",
       " 'areas': 4,\n",
       " 'as': 3,\n",
       " 'at': 1,\n",
       " 'aurora': 10,\n",
       " 'be': 1,\n",
       " 'brief': 1,\n",
       " 'broken': 1,\n",
       " 'can': 1,\n",
       " 'capabilities': 2,\n",
       " 'capability': 1,\n",
       " 'caps': 1,\n",
       " 'central': 1,\n",
       " 'changes': 1,\n",
       " 'closely': 1,\n",
       " 'com': 2,\n",
       " 'companies': 1,\n",
       " 'congestion': 1,\n",
       " 'consolidated': 1,\n",
       " 'contact': 1,\n",
       " 'continue': 1,\n",
       " 'continues': 1,\n",
       " 'customers': 1,\n",
       " 'data': 1,\n",
       " 'database': 3,\n",
       " 'databases': 2,\n",
       " 'day': 3,\n",
       " 'demo': 1,\n",
       " 'demos': 1,\n",
       " 'detail': 1,\n",
       " 'discount': 1,\n",
       " 'doc': 1,\n",
       " 'due': 1,\n",
       " 'east': 1,\n",
       " 'eastern': 2,\n",
       " 'either': 1,\n",
       " 'energy': 1,\n",
       " 'enhancements': 5,\n",
       " 'epis': 4,\n",
       " 'ercot': 1,\n",
       " 'excluding': 1,\n",
       " 'fax': 1,\n",
       " 'feels': 1,\n",
       " 'few': 1,\n",
       " 'file': 1,\n",
       " 'follows': 1,\n",
       " 'for': 4,\n",
       " 'free': 1,\n",
       " 'friends': 1,\n",
       " 'full': 1,\n",
       " 'general': 1,\n",
       " 'get': 1,\n",
       " 'ground': 1,\n",
       " 'grow': 1,\n",
       " 'have': 1,\n",
       " 'help': 1,\n",
       " 'how': 2,\n",
       " 'hydro': 1,\n",
       " 'i': 2,\n",
       " 'idea': 1,\n",
       " 'improved': 1,\n",
       " 'in': 2,\n",
       " 'inc': 1,\n",
       " 'including': 1,\n",
       " 'information': 2,\n",
       " 'interface': 1,\n",
       " 'into': 2,\n",
       " 'ipp': 1,\n",
       " 'iso': 1,\n",
       " 'it': 1,\n",
       " 'iv': 1,\n",
       " 'last': 1,\n",
       " 'launch': 1,\n",
       " 'license': 2,\n",
       " 'licenses': 1,\n",
       " 'licensing': 1,\n",
       " 'limit': 1,\n",
       " 'limited': 1,\n",
       " 'll': 1,\n",
       " 'look': 1,\n",
       " 'lot': 1,\n",
       " 'made': 1,\n",
       " 'manager': 1,\n",
       " 'marginal': 1,\n",
       " 'market': 5,\n",
       " 'me': 1,\n",
       " 'meet': 1,\n",
       " 'model': 2,\n",
       " 'modeled': 1,\n",
       " 'modeling': 3,\n",
       " 'more': 1,\n",
       " 'most': 1,\n",
       " 'moving': 1,\n",
       " 'multiple': 2,\n",
       " 'needs': 2,\n",
       " 'new': 6,\n",
       " 'no': 1,\n",
       " 'notification': 1,\n",
       " 'now': 2,\n",
       " 'number': 1,\n",
       " 'of': 9,\n",
       " 'offer': 4,\n",
       " 'official': 1,\n",
       " 'on': 1,\n",
       " 'operations': 2,\n",
       " 'options': 1,\n",
       " 'or': 2,\n",
       " 'our': 2,\n",
       " 'over': 1,\n",
       " 'pc': 2,\n",
       " 'pcs': 4,\n",
       " 'period': 1,\n",
       " 'please': 1,\n",
       " 'price': 1,\n",
       " 'priced': 1,\n",
       " 'pricing': 1,\n",
       " 'procedural': 1,\n",
       " 'projects': 1,\n",
       " 'pumped': 1,\n",
       " 'regarding': 1,\n",
       " 'release': 1,\n",
       " 'released': 1,\n",
       " 'reporting': 3,\n",
       " 'resource': 3,\n",
       " 'resources': 3,\n",
       " 'responded': 1,\n",
       " 'risk': 1,\n",
       " 'run': 1,\n",
       " 's': 4,\n",
       " 'sales': 1,\n",
       " 'scripting': 1,\n",
       " 'serve': 1,\n",
       " 'several': 1,\n",
       " 'single': 1,\n",
       " 'site': 2,\n",
       " 'software': 1,\n",
       " 'some': 1,\n",
       " 'speak': 1,\n",
       " 'special': 1,\n",
       " 'specific': 1,\n",
       " 'speed': 1,\n",
       " 'spoken': 1,\n",
       " 'stacks': 1,\n",
       " 'storage': 1,\n",
       " 'take': 1,\n",
       " 'tel': 1,\n",
       " 'that': 2,\n",
       " 'the': 12,\n",
       " 'thinks': 1,\n",
       " 'this': 2,\n",
       " 'those': 1,\n",
       " 'time': 1,\n",
       " 'to': 8,\n",
       " 'todd': 2,\n",
       " 'tomorrow': 1,\n",
       " 'transfers': 1,\n",
       " 'transmission': 1,\n",
       " 'trial': 1,\n",
       " 'two': 1,\n",
       " 'u': 1,\n",
       " 'unlimited': 2,\n",
       " 'update': 1,\n",
       " 'updated': 3,\n",
       " 'use': 2,\n",
       " 'user': 4,\n",
       " 'users': 4,\n",
       " 'v': 1,\n",
       " 'vb': 1,\n",
       " 've': 2,\n",
       " 'version': 5,\n",
       " 'via': 1,\n",
       " 'want': 1,\n",
       " 'we': 8,\n",
       " 'weeks': 1,\n",
       " 'what': 2,\n",
       " 'wheeler': 1,\n",
       " 'will': 1,\n",
       " 'with': 5,\n",
       " 'worked': 1,\n",
       " 'wscc': 1,\n",
       " 'www': 1,\n",
       " 'x': 1,\n",
       " 'york': 1,\n",
       " 'you': 3,\n",
       " 'your': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Frequency. This is the example given from the dataset kaggle (way 1). This prints in any order #\n",
    "def select_vocabulary(dataframe, topN=100):\n",
    "    #for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.items() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "    print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "    print(\"-------------------------\")\n",
    "    for i in range(15):\n",
    "        print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    print(\"...\\n\")\n",
    "    return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(filtered_sentence)\n",
    "word2ind   = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "#print('----')\n",
    "#print (word2ind)\n",
    "#print('----')\n",
    "\n",
    "# 7. Frequency. This is another example (way 2). The frequencies get ordered in alphabetical order \n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vec = CountVectorizer()\n",
    "#data = vec.fit_transform(filtered_sentence).toarray()\n",
    "#vocab = vec.get_feature_names()\n",
    "#np.clip(data, 0, 1, out=data)\n",
    "#dist = np.sum(data, axis=0)\n",
    "#print ('--This is the frequency in order--')\n",
    "#for tag, count in zip(vocab, dist):\n",
    "    #print(count,tag)\n",
    "\n",
    "from collections import Counter\n",
    "t = np.random.choice(df['text'])\n",
    "\n",
    "dict(Counter(t.split()))\n",
    "    \n",
    "# Do more cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
