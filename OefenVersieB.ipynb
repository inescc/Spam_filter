{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import packages\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--dirty--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 2. Import data needed\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "#df_pos = df[df['spam']==1]\n",
    "\n",
    "# 3. Start to clean the data \n",
    "#text = np.random.choice(df['text'])\n",
    "#path = os.getcwd()\n",
    "#os.chdir(path)\n",
    "#train_df = pd.read_csv('./emails.train.csv', nrows=20, delimiter=',')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #tokens without special character\n",
    "stop_words = set(stopwords.words('english')) #stop words remover\n",
    "ps = PorterStemmer() #stemming words\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizing words\n",
    "\n",
    "del_words = ['subject', '_']\n",
    "\n",
    "print('--dirty--')\n",
    "#print(text)\n",
    "\n",
    "#print(stop_words)\n",
    "\n",
    "def select_vocabulary(dataframe, topN=1000):\n",
    "    #for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "        word_tokens = tokenizer.tokenize(line) # tokenize all the words in line\n",
    "        #words = line.split()\n",
    "        for word in word_tokens: \n",
    "            if word not in stop_words:\n",
    "                if word not in del_words: # we check if the current words were looking at are not in the stopwords, we wnat to use as ft\n",
    "                    #if word != 'Subject':\n",
    "                        word = ps.stem(word) # only for the words that I am going to use, with \"word\"we overwrite waht used to be the word that is stemmed\n",
    "                        #word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.items() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    #word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "#     print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "#     print(\"-------------------------\")\n",
    "#     for i in range(150):\n",
    "#         print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    #print(\"...\\n\")\n",
    "    #return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(df['text'])\n",
    "print(vocabulary)\n",
    "#word2ind   = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "#print(word2ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting feature for train ...\n",
      "   0 / 4021 \n",
      "1000 / 4021 \n",
      "2000 / 4021 \n",
      "3000 / 4021 \n",
      "4000 / 4021 \n",
      "4020 / 4021 \n",
      "[[  0.   0.   0. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [  4.   4.   2. ...,   0.   0.   0.]\n",
      " [  0.  11.   7. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]]\n",
      "Extracting feature for test ...\n",
      "   0 / 1707 \n",
      "1000 / 1707 \n",
      "1706 / 1707 \n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "def extract_Bag_of_Word_feature(dataframe):\n",
    "    BoWs = np.zeros((len(dataframe), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    for i, line in enumerate(dataframe):\n",
    "        for word in line.split():   ## these are the not stemmed words\n",
    "            word = ps.stem(word)    # looked for the stemmed words in our dict cause we stemmed them \n",
    "            word_ind = word2ind.get(word, -1) #looks in dict\n",
    "            if(word_ind>=0):\n",
    "                BoWs[i, word_ind] += 1\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "    print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "            \n",
    "    return BoWs\n",
    "\n",
    "\n",
    "# Make sure use cleaned version\n",
    "train = pd.read_csv('./emails.train.csv')\n",
    "test  = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test['spam']\n",
    "\n",
    "print(\"Extracting feature for train ...\")\n",
    "X_train = extract_Bag_of_Word_feature(train['text']) # these are our features\n",
    "print(X_train)\n",
    "\n",
    "print(\"Extracting feature for test ...\")\n",
    "X_test  = extract_Bag_of_Word_feature(test[ 'text']) # this is prediction \n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Performance of: SVM =======\n",
      "Metric[Accuracy            ]  0.967194\n",
      "Metric[Average Precision   ]  0.983188\n",
      "[output] solution.SVM.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, Y_train) # fit is for our training data, the extracted bag of words\n",
    "\n",
    "def eval(model, X_test, Y_test, method=''):\n",
    "    print(\"====== Performance of: {method} =======\".format(method=method))\n",
    "    \n",
    "    # Predict decision labels.\n",
    "    Y_pred  = model.predict(X_test)  #we need this \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                              score=accuracy_score(Y_test, Y_pred)) )\n",
    "\n",
    "    # Predict confidence scores.\n",
    "    Y_score = model.decision_function(X_test)    \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score)) )\n",
    "\n",
    "    # write to submit format\n",
    "    outf = 'solution.%s.csv'% method\n",
    "    with open( outf, 'w') as f:\n",
    "        f.write('id,spam\\n')\n",
    "        for i in range(len(Y_pred)):\n",
    "            # print test['id'][0]\n",
    "            f.write('%s,%s\\n' % (test['id'][i], Y_pred[i]) )\n",
    "    print(\"[output] \"+outf)\n",
    "    \n",
    "    \n",
    "# evaluate current model\n",
    "eval(model, X_test, Y_test, method='SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
